<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>hugo-site on Saurav Maheshkar</title><link>https://sauravmaheshkar.github.io/tags/hugo-site/</link><description>Recent content in hugo-site on Saurav Maheshkar</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 31 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://sauravmaheshkar.github.io/tags/hugo-site/index.xml" rel="self" type="application/rss+xml"/><item><title>Rigging the Lottery</title><link>https://sauravmaheshkar.github.io/blog/compression-series/01/01-rigging-the-lottery/</link><pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate><guid>https://sauravmaheshkar.github.io/blog/compression-series/01/01-rigging-the-lottery/</guid><description>Check out the Paper
Key Takeaway: The authors introduce RigL - an algorithm for training sparse neural networks while maintaining memory and computational cost proportional to density of the network. RigL achieves higher quality than all previous techniques for a given computational cost. RigL can find more accurate models than the current best dense-to-sparse training algorithms. Provide insight as to why allowing the topology of non-zero weights to change over the course of training aids optimization.</description></item><item><title>Optimal Brain Damage</title><link>https://sauravmaheshkar.github.io/blog/compression-series/02/02-optimal-brain-damage/</link><pubDate>Mon, 29 Mar 2021 00:00:00 +0000</pubDate><guid>https://sauravmaheshkar.github.io/blog/compression-series/02/02-optimal-brain-damage/</guid><description>Check out the Paper
Key Takeaway: The basic idea is to use second-order derivative information to make a tradeoff between network complexity and training set error.
Introduction The Authors used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing the unimportant weights from a network, several improvements can be expected:
Better Generalisation Fewer Training samples required Improved speed of learning and/or classification Most successful applications of NN learning to real-world problems have been achieved using highly structured networks of large sizes.</description></item><item><title>Compression Series</title><link>https://sauravmaheshkar.github.io/blog/compression-series/</link><pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate><guid>https://sauravmaheshkar.github.io/blog/compression-series/</guid><description/></item></channel></rss>