<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Paper Summaries on Saurav Maheshkar</title><link>https://sauravm.netlify.app/categories/paper-summaries/</link><description>Recent content in Paper Summaries on Saurav Maheshkar</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 31 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://sauravm.netlify.app/categories/paper-summaries/index.xml" rel="self" type="application/rss+xml"/><item><title>Rigging the Lottery</title><link>https://sauravm.netlify.app/blog/compression-series/01/01-rigging-the-lottery/</link><pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/blog/compression-series/01/01-rigging-the-lottery/</guid><description>Check out the Paper
Key Takeaway: The authors introduce RigL - an algorithm for training sparse neural networks while maintaining memory and computational cost proportional to density of the network. RigL achieves higher quality than all previous techniques for a given computational cost. RigL can find more accurate models than the current best dense-to-sparse training algorithms. Provide insight as to why allowing the topology of non-zero weights to change over the course of training aids optimization.</description></item><item><title>Every Model is a Kernel Machine</title><link>https://sauravm.netlify.app/blog/kernel-machine/</link><pubDate>Fri, 19 Feb 2021 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/blog/kernel-machine/</guid><description>Introduction We all know how poorly understood deep learning is, in recent times fields such as Explainability and Interpretability have emerged highlighting the black box that are deep nets. In contrast, kernel machines are based on a well-developed mathematical theory but their performance lacks behind that of deep networks. The most used algorithm for training deep networks is undoubtedly Gradient Descent. In this paper the author shows that every model learnt using this method, regardless of its architecture, is approximately equivalent tp a kernel machine ðŸ¤”.</description></item><item><title>Regularized Greedy Forest</title><link>https://sauravm.netlify.app/blog/regularized-greedy-forest/</link><pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/blog/regularized-greedy-forest/</guid><description>Check out the Paper
Key Takeaway: A new proposed method which learns decision forests via fully-corrective regularized greedy search using the underlying forest structure by defining regularizers that explicitly take advantage of individual tree structures.
Introduction A popular method to solve the problem of learning non-linear functions from data is through decision tree learning , which has an important advantage for handling heterogeneous data with ease when different features come from different sources.</description></item><item><title>Optimal Brain Damage</title><link>https://sauravm.netlify.app/blog/compression-series/02/02-optimal-brain-damage/</link><pubDate>Mon, 29 Mar 2021 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/blog/compression-series/02/02-optimal-brain-damage/</guid><description>Check out the Paper
Key Takeaway: The basic idea is to use second-order derivative information to make a tradeoff between network complexity and training set error.
Introduction The Authors used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing the unimportant weights from a network, several improvements can be expected:
Better Generalisation Fewer Training samples required Improved speed of learning and/or classification Most successful applications of NN learning to real-world problems have been achieved using highly structured networks of large sizes.</description></item><item><title>Compression Series</title><link>https://sauravm.netlify.app/blog/compression-series/</link><pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/blog/compression-series/</guid><description/></item></channel></rss>