<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üëãüèªSaurav Maheshkar on Saurav Maheshkar</title><link>https://sauravm.netlify.app/</link><description>Recent content in üëãüèªSaurav Maheshkar on Saurav Maheshkar</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 31 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://sauravm.netlify.app/index.xml" rel="self" type="application/rss+xml"/><item><title>Rigging the Lottery</title><link>https://sauravm.netlify.app/blog/compression-series/01/01-rigging-the-lottery/</link><pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/blog/compression-series/01/01-rigging-the-lottery/</guid><description>Check out the Paper
Key Takeaway: The authors introduce RigL - an algorithm for training sparse neural networks while maintaining memory and computational cost proportional to density of the network. RigL achieves higher quality than all previous techniques for a given computational cost. RigL can find more accurate models than the current best dense-to-sparse training algorithms. Provide insight as to why allowing the topology of non-zero weights to change over the course of training aids optimization.</description></item><item><title>Every Model is a Kernel Machine</title><link>https://sauravm.netlify.app/blog/kernel-machine/</link><pubDate>Fri, 19 Feb 2021 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/blog/kernel-machine/</guid><description>Introduction We all know how poorly understood deep learning is, in recent times fields such as Explainability and Interpretability have emerged highlighting the black box that are deep nets. In contrast, kernel machines are based on a well-developed mathematical theory but their performance lacks behind that of deep networks. The most used algorithm for training deep networks is undoubtedly Gradient Descent. In this paper the author shows that every model learnt using this method, regardless of its architecture, is approximately equivalent tp a kernel machine ü§î.</description></item><item><title>Introduction to Duck Typing and EAFP</title><link>https://sauravm.netlify.app/blog/duck-typing-eafp/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/blog/duck-typing-eafp/</guid><description>Duck-Typing is a extremely useful programming style, which truly makes python awesome. It enables us to &amp;ldquo;ignore&amp;rdquo; the object type and rather just check if the object contains the function or not.
Famously referred in the python documentation as:
&amp;ldquo;If it looks like a duck and quacks like a duck, it must be a duck&amp;rdquo;
If the codebase is well defined, this allows for flexibility by allowing polymorphic substitution.</description></item><item><title>Named Entity Recognition using Reformer</title><link>https://sauravm.netlify.app/blog/ner-reformer/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/blog/ner-reformer/</guid><description>Link to the Kaggle Kernel which is referenced in this post
Introduction Named entity recognition(NER) is the task to identify mentions of rigid designators from text belonging to predefined semantic types such as person, location, organization etc. NER always servers as the foundation of many natural language applications such as question answering, text summarization, and machine translation.
Despite the various definitions of NE(Named Entity), researchers have reached common consensus on the types of NEs to recognize.</description></item><item><title>Regularized Greedy Forest</title><link>https://sauravm.netlify.app/blog/regularized-greedy-forest/</link><pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/blog/regularized-greedy-forest/</guid><description>Check out the Paper
Key Takeaway: A new proposed method which learns decision forests via fully-corrective regularized greedy search using the underlying forest structure by defining regularizers that explicitly take advantage of individual tree structures.
Introduction A popular method to solve the problem of learning non-linear functions from data is through decision tree learning , which has an important advantage for handling heterogeneous data with ease when different features come from different sources.</description></item><item><title>Optimal Brain Damage</title><link>https://sauravm.netlify.app/blog/compression-series/02/02-optimal-brain-damage/</link><pubDate>Mon, 29 Mar 2021 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/blog/compression-series/02/02-optimal-brain-damage/</guid><description>Check out the Paper
Key Takeaway: The basic idea is to use second-order derivative information to make a tradeoff between network complexity and training set error.
Introduction The Authors used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing the unimportant weights from a network, several improvements can be expected:
Better Generalisation Fewer Training samples required Improved speed of learning and/or classification Most successful applications of NN learning to real-world problems have been achieved using highly structured networks of large sizes.</description></item><item><title>Stellarflare</title><link>https://sauravm.netlify.app/project/stellarflare/</link><pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/project/stellarflare/</guid><description> Tensorflow Based Framework for detecting Stellar Flares</description></item><item><title>X Ray Image Analysis</title><link>https://sauravm.netlify.app/project/xray-image-analysis/</link><pubDate>Sat, 10 Apr 2021 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/project/xray-image-analysis/</guid><description>In this project, I went over the entire pipeline of creating a Binary Image Classifier using Tensorflow. I covered, all aspects of the pipeline such as experimenting with different network architectures and comparing metrics, pruning and quantization of the model for faster inference and finally two methods of deploying such models. I trained a Convolutional Neural Network(Efficient Net) to recognize chest X-rays of people with pneumonia and then created a interactive web application.</description></item><item><title>Compressed DNNs Forget</title><link>https://sauravm.netlify.app/project/bias-compressed/</link><pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/project/bias-compressed/</guid><description>Reproducibility Study Current state-of-the-art models are famously huge and over-parameterized‚Äì‚Äìin fact, they contain way more parameters than the number of data points in the dataset. But in many ways, over-parameterization is behind the success of modern-day deep learning. Think about something like Switch Transformer having a trillion parameters or Vision Transformer-Huge having 632M parameters. These models require enormous amounts of computation and memory and that not only increases the infrastructure costs, but also makes deployment to resource-constrained environments such as mobile phones or smart devices challenging.</description></item><item><title>A campfire</title><link>https://sauravm.netlify.app/talk/campfire/</link><pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/talk/campfire/</guid><description>I&amp;rsquo;m really excited to give this talk! Stay tuned for video and slides.</description></item><item><title>A seedling</title><link>https://sauravm.netlify.app/talk/second-seedling/</link><pubDate>Fri, 01 Jan 2021 14:15:59 -0600</pubDate><guid>https://sauravm.netlify.app/talk/second-seedling/</guid><description/></item><item><title>A seedling</title><link>https://sauravm.netlify.app/talk/seedling/</link><pubDate>Mon, 13 Jan 2020 14:15:59 -0600</pubDate><guid>https://sauravm.netlify.app/talk/seedling/</guid><description/></item><item><title>Contact</title><link>https://sauravm.netlify.app/contact/</link><pubDate>Mon, 25 Feb 2019 13:38:41 -0600</pubDate><guid>https://sauravm.netlify.app/contact/</guid><description/></item><item><title>License</title><link>https://sauravm.netlify.app/license/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/license/</guid><description>My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.</description></item></channel></rss>