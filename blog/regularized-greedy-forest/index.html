<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.85.0"><title>Regularized Greedy Forest | Saurav Maheshkar</title><meta property="twitter:site" content="@MaheshkarSaurav"><meta property="twitter:creator" content="@MaheshkarSaurav"><meta name=description content="Notes on the paper Learning Nonlinear Functions Using Regularized Greedy Forest by Rie Johnson"><meta property="og:site_name" content="Saurav Maheshkar"><meta property="og:title" content="Regularized Greedy Forest | Saurav Maheshkar"><meta property="og:description" content="Notes on the paper Learning Nonlinear Functions Using Regularized Greedy Forest by Rie Johnson"><meta property="og:type" content="page"><meta property="og:url" content="https://sauravmaheshkar.github.io/blog/regularized-greedy-forest/"><meta property="og:locale" content="en"><meta property="og:image" content="https://sauravmaheshkar.github.io/blog/regularized-greedy-forest/featured.jpg"><meta property="twitter:card" content="summary_large_image"><meta name=twitter:image content="https://sauravmaheshkar.github.io/blog/regularized-greedy-forest/featured.jpg"><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta itemprop=name content="Regularized Greedy Forest"><meta itemprop=description content="Check out the Paper
Key Takeaway: A new proposed method which learns decision forests via fully-corrective regularized greedy search using the underlying forest structure by defining regularizers that explicitly take advantage of individual tree structures.
Introduction     A popular method to solve the problem of learning non-linear functions from data is through decision tree learning , which has an important advantage for handling heterogeneous data with ease when different features come from different sources."><meta itemprop=datePublished content="2020-09-27T00:00:00+00:00"><meta itemprop=dateModified content="2020-09-27T00:00:00+00:00"><meta itemprop=wordCount content="626"><meta itemprop=image content="https://sauravmaheshkar.github.io/blog/regularized-greedy-forest/featured.jpg"><meta itemprop=keywords content><!--[if IE]><script src=//html5shiv.googlecode.com/svn/trunk/html5.js></script><![endif]--><link rel="shortcut icon" href=/img/favicon.ico type=image/x-icon><link rel=icon href=/img/favicon.ico type=image/x-icon><link rel=stylesheet href=/style.main.min.9fe1917e3fd8cb643bf35aa0995479b67c8cee6eeca071b3bbe20d3248308e3b.css integrity="sha256-n+GRfj/Yy2Q781qgmVR5tnyM7m7soHGzu+INMkgwjjs=" media=screen><script src=/panelset.min.078a92db9bd3228df502db3d9e0453c3cf3d910abe3f8deca0ad196c7071ad41.js type=text/javascript></script><script src=/main.min.38a0323c5b0bbb611c4874ba2d8fdaba57d20cc2b0d704b30250c235ba8b6d49.js type=text/javascript></script></head><body><div class=grid-container><header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role=banner><nav class="site-nav db dt-l w-100" role=navigation><a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href=https://sauravmaheshkar.github.io/ title=Home><img src=/img/android-chrome-256x256.png class="dib db-l h2 w-auto" alt="Saurav Maheshkar"></a><div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked"><a class="link f6 f5-l dib pv1 ph2" href=/about/ title=About>About</a>
<a class="link f6 f5-l dib pv1 ph2 active" href=/blog/ title=Blog>Blog</a>
<a class="link f6 f5-l dib pv1 ph2" href=/project/ title="Project Portfolio">Projects</a>
<a class="link f6 f5-l dib pv1 ph2" href=https://sauravmaheshkar.substack.com/welcome title=Newsletter>Newsletter</a></div></nav></header><main class="page-main pa4" role=main><section class="page-content mw7 center"><article class="post-content pa0 ph4-l"><header class=post-header><h1 class="f1 lh-solid measure-narrow mb3 fw4">Regularized Greedy Forest</h1><p class="f6 measure lh-copy mv1">By Saurav Maheshkar in <a href=https://sauravmaheshkar.github.io/categories/paper-summaries>Paper Summaries</a></p><p class="f7 db mv0 ttu">September 27, 2020</p></header><section class="post-body pt5 pb4"><p><a href=https://arxiv.org/pdf/1109.0887.pdf target=_blank rel=noopener>Check out the Paper</a></p><p><strong>Key Takeaway:</strong> A new proposed method which learns decision forests via fully-corrective regularized greedy search using the underlying forest structure by defining regularizers that explicitly take advantage of individual tree structures.</p><h2 id=introduction>Introduction
<a href=#introduction><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p><img src=/img/regular-greedy-forest.jpg alt></p><p>A popular method to solve the problem of <em>learning non-linear functions from data</em> is through decision tree learning , which has an important advantage for handling heterogeneous data with ease when different features come from different sources.</p><p>However, a disadvantage of decision tree learning is that it does not generally achieve the most accurate
prediction performance, when compared to other methods. A remedy for this problem is through <strong>boosting</strong> where one builds an additive model of decision trees by sequentially building trees one by one. In general ‚Äúboosted decision trees‚Äù is regarded as the most effective off-the-shelf nonlinear learning method for a wide range of application problems.</p><p>In the boosted tree approach, one considers an additive model over multiple decision trees, and thus, we will
refer to the resulting function as a <em><strong>decision forest</strong></em>.</p><p>Due to the practical importance of boosted decision trees in applications, it is natural to ask whether one can design a more direct procedure that specifically learns decision forests without using a black-box decision tree learner under the wrapper. The purpose of doing so is that by directly taking advantage of the underlying tree
structure, we shall be able to design a more effective algorithm for learning the final nonlinear decision forest.</p><hr><p>We consider the problem of learning a single nonlinear function $h(x)$ on some input vector $x = [ x_{[1]}, &mldr;, x_{[d]} ] \in \mathbb{R}^d$ from a set of training examples. In supervised learning, we are given a set of input vectors $X = [x_1, &mldr;, x_n]$ with labels $Y = [y_1, &mldr;, y_m]$ (here <b>m may not equal to n</b>). Our training goal is to find a nonlinear prediction function $\hat{h}(x)$ from a function class $ H $ that minimizes a risk function.</p><p>$$\hat{h} = \min_{h \in H} L(h(X), Y)$$</p><p>$H$ ‚Üí is a pre-defined nonlinear function class $h(X) = [h(x_1), &mldr;, h(x_n)]$</p><p>$L$ ‚Üí General Loss Function</p><h2 id=notation-the-complex-part->Notation (The Complex Part üòÖ)
<a href=#notation-the-complex-part-><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><ul><li>A Forest is an ensemble of multiple decision trees $T_1, T_2, &mldr;, T_k$</li><li>Each tree edge $e$ is associated with a variable $k_e$ and threshold $t_e$ and denotes a decision</li><li>Mathematically, each node $v$ of the forest is associated with a decision rule of the form</li></ul><p>$$ b_{v}(x) = \prod_{j} I(x[i_j] \leq t_{i_{j}}) \prod_{k} I (x[i_k] > t_{i_{k}}) $$</p><ul><li>$h_{F}(x) = \sum_{v , \in F} \alpha_vb_v(x)$ , where $\alpha$ is the weight assigned to a node ( equal to zero for any internal node )</li></ul><p>Thus, regularized loss is of the form</p><p>$$ Q(F) = L(h_{F}(X), Y) + R(h_{F}) $$</p><h2 id=algorithmic-framework>Algorithmic Framework
<a href=#algorithmic-framework><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>Since, the exact optimum solution is difficult to find, we <strong>greedily select the basis functions and optimize the weights</strong>. It has two main components:</p><ul><li>Fix the weights, and change the structure of the forest (which changes basis functions) so that the loss is reduced the most.</li><li>Fix the structure of the forest, and change the weights so that loss is minimized.</li></ul><h2 id=tree-structured-regularization>Tree-structured regularization
<a href=#tree-structured-regularization><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>For any additive models over leaf sample nodes only, there always exist equivalent models over all the nodes of the same tree that produce the same output.</p><p>Our basic idea is that it is natural to give the same regularization penalty to all equivalent models defined on the same tree topology. One way to define a regularizer that satisfies this condition is to choose
a model of some desirable properties as the unique representation for all the equivalent models and define the regularization penalty based on this unique representation.</p><p>The following penalty functions could be choosed:</p><ul><li>$L_2$ regularization</li><li>Minimum-penalty regularization</li><li>Min-penalty regularization with sum-to-zero sibling constraints</li></ul><details closed class="f6 fw7 input-reset"><dl class="f6 lh-copy"><dt class=fw7>Posted on:</dt><dd class="fw5 ml0">September 27, 2020</dd></dl><dl class="f6 lh-copy"><dt class=fw7>Length:</dt><dd class="fw5 ml0">3 minute read, 626 words</dd></dl><dl class="f6 lh-copy"><dt class=fw7>Categories:</dt><dd class="fw5 ml0"><a href=https://sauravmaheshkar.github.io/categories/paper-summaries>Paper Summaries</a></dd></dl><dl class="f6 lh-copy"><dt class=fw7>See Also:</dt></dl></details></section><footer class=post-footer><div class="post-pagination dt w-100 mt4 mb2"><a class="prev dtc pr2 tl v-top fw6" href=https://sauravmaheshkar.github.io/blog/ner-reformer/>&larr; Named Entity Recognition using Reformer</a></div></footer></article><div class="post-comments pa0 pa4-l mt4"><script src=https://utteranc.es/client.js repo=apreshill/apero issue-term=pathname theme=boxy-light label="comments :crystal_ball:" crossorigin=anonymous async type=text/javascript></script></div></section></main><footer class="site-footer pv4 bt b--transparent ph5" role=contentinfo><nav class="db dt-l w-100"><p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">&copy; 2021 Weights and Biases, New Delhi
<span class=middot-divider></span>Made with <span xmlns:dct=http://purl.org/dc/terms/ property="dct:title"><a xmlns:dct=http://purl.org/dc/terms/ href=https://github.com/hugo-apero/ rel=dct:source>Hugo Ap√©ro</a></span>.<br>Based on <span xmlns:dct=http://purl.org/dc/terms/ property="dct:title"><a xmlns:dct=http://purl.org/dc/terms/ href=https://github.com/formspree/blogophonic-hugo rel=dct:source>Blogophonic</a></span> by <a xmlns:cc=http://creativecommons.org/ns# href=https://formspree.io property="cc:attributionName" rel=cc:attributionurl>Formspree</a>.</p><div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0"><div class=social-icon-links aria-hidden=true><a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=https://github.com/SauravMaheshkar title=github target=_blank rel=noopener><i class="fab fa-github fa-lg fa-fw"></i></a>
<a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=https://www.instagram.com/sauravvmaheshkar/ title=instagram target=_blank rel=noopener><i class="fab fa-instagram fa-lg fa-fw"></i></a>
<a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=https://twitter.com/MaheshkarSaurav title=twitter target=_blank rel=noopener><i class="fab fa-twitter fa-lg fa-fw"></i></a>
<a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=https://orcid.org/0000-0002-5371-4651 title=orcid target=_blank rel=noopener><i class="ai ai-orcid fa-lg fa-fw"></i></a>
<a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=/blog/index.xml title=rss><i class="fas fa-rss fa-lg fa-fw"></i></a></div></div><div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0"><a class="dib pv1 ph2 link" href=/license/ title=License>License</a>
<a class="dib pv1 ph2 link" href=/contact/ title="Contact form">Contact</a></div></nav></footer></div></body></html>