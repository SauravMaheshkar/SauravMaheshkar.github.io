<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.85.0"><title>Optimal Brain Damage | Saurav Maheshkar</title><meta property="twitter:site" content="@MaheshkarSaurav"><meta property="twitter:creator" content="@MaheshkarSaurav"><meta name=description content="Notes on the paper Optimal Brain Damage by Yann LeCun, John S. Denker and Sara A. Solla"><meta property="og:site_name" content="Saurav Maheshkar"><meta property="og:title" content="Optimal Brain Damage | Saurav Maheshkar"><meta property="og:description" content="Notes on the paper Optimal Brain Damage by Yann LeCun, John S. Denker and Sara A. Solla"><meta property="og:type" content="page"><meta property="og:url" content="https://sauravmaheshkar.github.io/blog/compression-series/02/02-optimal-brain-damage/"><meta property="og:locale" content="en"><meta property="og:image" content="https://sauravmaheshkar.github.io/blog/compression-series/sidebar-compression.jpg"><meta property="twitter:card" content="summary_large_image"><meta name=twitter:image content="https://sauravmaheshkar.github.io/blog/compression-series/sidebar-compression.jpg"><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta itemprop=name content="Optimal Brain Damage"><meta itemprop=description content="Check out the Paper
Key Takeaway: The basic idea is to use second-order derivative information to make a tradeoff between network complexity and training set error.
Introduction     The Authors used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing the unimportant weights from a network, several improvements can be expected:
 Better Generalisation Fewer Training samples required Improved speed of learning and/or classification  Most successful applications of NN learning to real-world problems have been achieved using highly structured networks of large sizes."><meta itemprop=datePublished content="2021-03-29T00:00:00+00:00"><meta itemprop=dateModified content="2021-03-29T00:00:00+00:00"><meta itemprop=wordCount content="572"><meta itemprop=keywords content="hugo-site,"><!--[if IE]><script src=//html5shiv.googlecode.com/svn/trunk/html5.js></script><![endif]--><link rel="shortcut icon" href=/img/favicon.ico type=image/x-icon><link rel=icon href=/img/favicon.ico type=image/x-icon><link rel=stylesheet href=/style.main.min.9fe1917e3fd8cb643bf35aa0995479b67c8cee6eeca071b3bbe20d3248308e3b.css integrity="sha256-n+GRfj/Yy2Q781qgmVR5tnyM7m7soHGzu+INMkgwjjs=" media=screen><script src=/panelset.min.078a92db9bd3228df502db3d9e0453c3cf3d910abe3f8deca0ad196c7071ad41.js type=text/javascript></script><script src=/main.min.38a0323c5b0bbb611c4874ba2d8fdaba57d20cc2b0d704b30250c235ba8b6d49.js type=text/javascript></script></head><body><div class="grid-container single-series"><header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role=banner><nav class="site-nav db dt-l w-100" role=navigation><a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href=https://sauravmaheshkar.github.io/ title=Home><img src=/img/android-chrome-256x256.png class="dib db-l h2 w-auto" alt="Saurav Maheshkar"></a><div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked"><a class="link f6 f5-l dib pv1 ph2" href=/about/ title=About>About</a>
<a class="link f6 f5-l dib pv1 ph2 active" href=/blog/ title=Blog>Blog</a>
<a class="link f6 f5-l dib pv1 ph2" href=/project/ title="Project Portfolio">Projects</a>
<a class="link f6 f5-l dib pv1 ph2" href=https://sauravmaheshkar.substack.com/welcome title=Newsletter>Newsletter</a></div></nav></header><main class="page-main pa4" role=main><section class="page-content mw7 center"><article class="post-content pa0 pl3-l"><header class=post-header><h1 class="f1 lh-solid measure-narrow mb3 fw4">Optimal Brain Damage</h1><p class="f6 measure lh-copy mv1">By Saurav Maheshkar in <a href=https://sauravmaheshkar.github.io/categories/paper-summaries>Paper Summaries</a></p><p class="f7 db mv0 ttu">March 29, 2021</p></header><section class="post-body pt5 pb4"><p><a href=https://papers.nips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf target=_blank rel=noopener>Check out the Paper</a></p><p><strong>Key Takeaway:</strong> The basic idea is to use second-order derivative information to make a tradeoff between network complexity and training set error.</p><h2 id=introduction>Introduction
<a href=#introduction><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>The Authors used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing the unimportant weights from a network, several improvements can be expected:</p><ul><li>Better Generalisation</li><li>Fewer Training samples required</li><li>Improved speed of learning and/or classification</li></ul><p>Most successful applications of NN learning to real-world problems have been achieved using highly structured networks of large sizes. As applications become more complex, the networks would presumably become even larger and more structured. The authors <span style=color:orange><strong>propose Optimal Brain Damage (OBD)</strong></span> for reducing the size of a learning network by selectively deleting weights.</p><p>The basic idea of OBD is that it is possible to take a perfectly reasonable network, delete half (or more) of the weights and wind up with a network that works just as well, <em>or better</em>. It is known from theory and experience that, for a fixed amount of training data, networks with too many weights do not generalize well. On the other hand, networks with too few weights will not have enough power to represent the data accurately. <span style=color:orange><strong>The best generalization is obtained by trading off the training error and the network complexity.</strong></span></p><h2 id=possible-approaches>Possible Approaches
<a href=#possible-approaches><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>One such method could be to minimize a cost function composed of two terms, the ordinary training error, plus some measure of the network complexity.</p><p>But a simpler strategy exists which doesn&rsquo;t involve priori or heurisitc information i.e. to delete parameters with small <span style=color:orange><strong>&ldquo;saliency&rdquo;</strong></span> (those whose deletion has the least effect on the training error). Other things being equal, small-magnitude parameters will have the least saliency, so a reasonable initial strategy is to train the network and delete small-magnitude parameters. After deletion, the network is retrained. Two drawbacks of these techniques are that they require fine-tuning of the &ldquo;pruning&rdquo; coefficients to avoid catastrophic effects, and also that the learning process is significantly slowed down.</p><h2 id=optimal-brain-damage>Optimal Brain Damage
<a href=#optimal-brain-damage><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>Objective functions play a central role, therefore it is more than reasonable to define the saliency of a parameter to be the change in objective function caused by deleting some parameter. It wouldbe prohibitively laborious to evaluate the saliency directly from this definition, i.e. by temporarily deleting each parameter and re-evaluating the objective function</p><p>However, it is possible to construct a local model of the error function and analytically predict the effect of perturbating the parameter vector. The objective function $E$ is approximated using Taylor Series. Here, the perturbation $ \mathcal{L} $ of the parameter vector will change the objective function by :</p><p>$$
\delta{E} = \sum_{i} g_i , \delta{u}_i + \frac{1}{2}\sum_{i} h_{ii} \delta {u_{i}}^2 + \frac{1}{2} \sum_{i \neq j} h_{ij} \delta{u}_{i} \delta{u}_{j} + O(||\mathcal{L}||^{3})
$$</p><p>The goal is to find a set of parameters whose deletion will cause the least increase of $ E $. The problem is computationally expensive in general cases, therefore we introduce some approximations. After using diagonal, extremal and local minimum approximation, this equation simplifies to:</p><p>$$
\delta{E} = \frac{1}{2} \sum_{i} h_{ii} \delta{u}^2_{j}
$$</p><h2 id=the-obd-recipe>The OBD Recipe
<a href=#the-obd-recipe><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><ol><li>Choose a reasonable network architecture</li><li>Train the network until a reasonable solution is obtained</li><li>Compute the second derivatives $ h_{kk} $ for each parameter</li><li>Compute the saliences for each parameter: $ s_k = h_{kk}u^2_k/2 $</li><li>Sort the parameters the saliency amd delete some low-saliency parameters.</li><li>Iterate to step 2</li></ol></section><footer class=post-footer><div class="post-pagination dt w-100 mt4 mb2"><a class="prev dtc pr2 tl v-top fw6" href=https://sauravmaheshkar.github.io/blog/compression-series/01/01-rigging-the-lottery/>&larr; Rigging the Lottery</a></div></footer></article></section></main><aside class=page-sidebar role=complementary><img src=/blog/compression-series/sidebar-compression.jpg class="db ma0"><div class="blog-info ph4 pt4 pb4 pb0-l"><h1 class=f3>Compression Series</h1><p class="f6 lh-copy measure">Paper Summaries and Notes on Neural Network Compression Techniques</p><p class="f7 measure lh-copy i mh0-l">Written by Saurav Maheshkar</p></div><div class="flex items-start sticky ph4 pb4"><div class="w-two-thirds w-50-l ph0"><h2 class="mt3 mb3 f5 fw7 ttu tracked"><a class="no-underline dim" href=/blog/compression-series/>In this series</a></h2><nav id=SeriesTableOfContents><ul><li hugo-nav=/blog/compression-series/01/01-rigging-the-lottery/><a href=https://sauravmaheshkar.github.io/blog/compression-series/01/01-rigging-the-lottery/>Rigging the Lottery</a></li><li class=active hugo-nav=/blog/compression-series/02/02-optimal-brain-damage/><a href=https://sauravmaheshkar.github.io/blog/compression-series/02/02-optimal-brain-damage/>Optimal Brain Damage</a></li></ul></nav></div><details open id=PageTableOfContents><summary><h2 class="mv0 f5 fw7 ttu tracked dib">On this page</h2></summary><div class="pl2 pr0 mh0"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#possible-approaches>Possible Approaches</a></li><li><a href=#optimal-brain-damage>Optimal Brain Damage</a></li><li><a href=#the-obd-recipe>The OBD Recipe</a></li></ul></nav></div></details></div></aside><footer class="site-footer pv4 bt b--transparent ph5" role=contentinfo><nav class="db dt-l w-100"><p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">&copy; 2021 Weights and Biases, New Delhi
<span class=middot-divider></span>Made with <span xmlns:dct=http://purl.org/dc/terms/ property="dct:title"><a xmlns:dct=http://purl.org/dc/terms/ href=https://github.com/hugo-apero/ rel=dct:source>Hugo Apéro</a></span>.<br>Based on <span xmlns:dct=http://purl.org/dc/terms/ property="dct:title"><a xmlns:dct=http://purl.org/dc/terms/ href=https://github.com/formspree/blogophonic-hugo rel=dct:source>Blogophonic</a></span> by <a xmlns:cc=http://creativecommons.org/ns# href=https://formspree.io property="cc:attributionName" rel=cc:attributionurl>Formspree</a>.</p><div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0"><div class=social-icon-links aria-hidden=true><a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=https://github.com/SauravMaheshkar title=github target=_blank rel=noopener><i class="fab fa-github fa-lg fa-fw"></i></a>
<a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=https://www.instagram.com/sauravvmaheshkar/ title=instagram target=_blank rel=noopener><i class="fab fa-instagram fa-lg fa-fw"></i></a>
<a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=https://twitter.com/MaheshkarSaurav title=twitter target=_blank rel=noopener><i class="fab fa-twitter fa-lg fa-fw"></i></a>
<a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=https://orcid.org/0000-0002-5371-4651 title=orcid target=_blank rel=noopener><i class="ai ai-orcid fa-lg fa-fw"></i></a>
<a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=/blog/index.xml title=rss><i class="fas fa-rss fa-lg fa-fw"></i></a></div></div><div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0"><a class="dib pv1 ph2 link" href=/license/ title=License>License</a>
<a class="dib pv1 ph2 link" href=/contact/ title="Contact form">Contact</a></div></nav></footer></div></body></html>