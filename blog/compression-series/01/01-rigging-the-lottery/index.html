<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.85.0"><title>Rigging the Lottery | Saurav Maheshkar</title><meta property="twitter:site" content="@MaheshkarSaurav"><meta property="twitter:creator" content="@MaheshkarSaurav"><meta name=description content="Notes on the paper Rigging the Lottery Making All Tickets Winners by Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro and Erich Elsen"><meta property="og:site_name" content="Saurav Maheshkar"><meta property="og:title" content="Rigging the Lottery | Saurav Maheshkar"><meta property="og:description" content="Notes on the paper Rigging the Lottery Making All Tickets Winners by Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro and Erich Elsen"><meta property="og:type" content="page"><meta property="og:url" content="https://sauravmaheshkar.github.io/blog/compression-series/01/01-rigging-the-lottery/"><meta property="og:locale" content="en"><meta property="og:image" content="https://sauravmaheshkar.github.io/blog/compression-series/sidebar-compression.jpg"><meta property="twitter:card" content="summary_large_image"><meta name=twitter:image content="https://sauravmaheshkar.github.io/blog/compression-series/sidebar-compression.jpg"><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta itemprop=name content="Rigging the Lottery"><meta itemprop=description content="Check out the Paper
Key Takeaway: The authors introduce RigL - an algorithm for training sparse neural networks while maintaining memory and computational cost proportional to density of the network. RigL achieves higher quality than all previous techniques for a given computational cost. RigL can find more accurate models than the current best dense-to-sparse training algorithms. Provide insight as to why allowing the topology of non-zero weights to change over the course of training aids optimization."><meta itemprop=datePublished content="2021-03-31T00:00:00+00:00"><meta itemprop=dateModified content="2021-03-31T00:00:00+00:00"><meta itemprop=wordCount content="723"><meta itemprop=keywords content="hugo-site,"><!--[if IE]><script src=//html5shiv.googlecode.com/svn/trunk/html5.js></script><![endif]--><link rel="shortcut icon" href=/img/favicon.ico type=image/x-icon><link rel=icon href=/img/favicon.ico type=image/x-icon><link rel=stylesheet href=/style.main.min.9fe1917e3fd8cb643bf35aa0995479b67c8cee6eeca071b3bbe20d3248308e3b.css integrity="sha256-n+GRfj/Yy2Q781qgmVR5tnyM7m7soHGzu+INMkgwjjs=" media=screen><script src=/panelset.min.078a92db9bd3228df502db3d9e0453c3cf3d910abe3f8deca0ad196c7071ad41.js type=text/javascript></script><script src=/main.min.38a0323c5b0bbb611c4874ba2d8fdaba57d20cc2b0d704b30250c235ba8b6d49.js type=text/javascript></script></head><body><div class="grid-container single-series"><header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role=banner><nav class="site-nav db dt-l w-100" role=navigation><a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href=https://sauravmaheshkar.github.io/ title=Home><img src=/img/android-chrome-256x256.png class="dib db-l h2 w-auto" alt="Saurav Maheshkar"></a><div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked"><a class="link f6 f5-l dib pv1 ph2" href=/about/ title=About>About</a>
<a class="link f6 f5-l dib pv1 ph2 active" href=/blog/ title=Blog>Blog</a>
<a class="link f6 f5-l dib pv1 ph2" href=/project/ title="Project Portfolio">Projects</a>
<a class="link f6 f5-l dib pv1 ph2" href=https://sauravmaheshkar.substack.com/welcome title=Newsletter>Newsletter</a></div></nav></header><main class="page-main pa4" role=main><section class="page-content mw7 center"><article class="post-content pa0 pl3-l"><header class=post-header><h1 class="f1 lh-solid measure-narrow mb3 fw4">Rigging the Lottery</h1><p class="f6 measure lh-copy mv1">By Saurav Maheshkar in <a href=https://sauravmaheshkar.github.io/categories/paper-summaries>Paper Summaries</a></p><p class="f7 db mv0 ttu">March 31, 2021</p></header><section class="post-body pt5 pb4"><p><a href=https://arxiv.org/abs/1911.11134 target=_blank rel=noopener>Check out the Paper</a></p><p><strong>Key Takeaway:</strong> The authors introduce RigL - an algorithm for training sparse neural networks while maintaining memory and computational cost proportional to density of the network. RigL achieves higher quality than all previous techniques for a given computational cost. RigL can find more accurate models than the current best dense-to-sparse training algorithms. Provide insight as to why allowing the topology of non-zero weights to change over the course of training aids optimization.</p><h2 id=introduction>Introduction
<a href=#introduction><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>Many applications require sparse neural networks due to space or inference time restrictions. There is a large body of work on training dense networks to yield sparse networks for inference, but this <strong>limits the size of the largest trainable sparse model to that of the largest trainable dense model</strong>. In this paper, the authors introduce a method to train sparse neural networks with a fixed parameter count annd a fixed computational cost throughout training without sacrificing accuracy.</p><p>This method updates the topology of the sparse network during training by using parameter magnitudes and infrequent gradient calculations.</p><h2 id=current-limitations>Current Limitations
<a href=#current-limitations><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>Currently, the most accurate sparse models are obtained with techniques that require, at a minimum, the cost of training a dense model in terms of memory and FLOPs. This paradigm has 2 main limitations:</p><ul><li>The maximum size of sparse models is limited to the largest dense model that can be trained. Even if sparse models are more parameter efficient, we can&rsquo;t use pruning to train models that are larger and more accurate than the largest possible dense models.</li><li>It is inefficient. Large amounts of computation must be performed for parameters that are zero valued or that will be zero during inference. Additionally, it remains unknown if the performance of the current best pruning algorithms is an upper bouond on the quality of sparse models.</li></ul><p>The Lottery Ticket Hypothesis hypothesized that if we can find a sparse neural network with iterative pruning, then we can train that sparse neural network from scratch, to the same level of accuracy, by starting from the initial conditions.</p><h2 id=the-rigl-method>The RigL Method
<a href=#the-rigl-method><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p><img src=/img/rigl/rigl.png alt=alt></p><p>RigL starts with a random sparse network, and at regularly spaced intervals it removes a fraction of connections based on their magnitudes and activates new ones using instantaneous gradient information. There are 4 main parts of the algorithm:</p><ol><li>Sparsity Distribution</li><li>Update Schedule</li><li>Drop Criterion</li><li>Grow Criterion</li></ol><h3 id=notation>Notation
<a href=#notation><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h3><ul><li>Given a dataset $D$ with individual samples $x_i$ and targets $y_i$ , the aim is to minimize the loss function $\sum_{i} L (f_{\theta}(x_i), y_i)$ , where $f_{\theta}(.)$ is a neural network with parameters $\theta \in \mathbb{R}^{N}$.</li><li>Parameters of the $l^{th}$ layer are denoted with $\theta^{l}$ which is a length $N^{l}$ vector.</li><li>A sparse layer keeps only a fraction $s^{l} \in (0,1)$ of its connections and parameterized with vector $\theta^{l}$ of length $(1 - s^{l})N^{l}$.</li><li>The overall sparsity of the network is defined as the ratio of zeros to the total parameter count $S = \frac{\sum_{l} s^{l} N^{l} }{N}$</li></ul><h3 id=sparsity-distribution>Sparsity Distribution
<a href=#sparsity-distribution><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h3><p>The following 3 strategies were considered:</p><ol><li><strong>Uniform:</strong> The sparsity $s^{l}$ of each individual layer is equal to the total sparsity $S$.</li><li><strong>Erdos-Renyi:</strong> $s^{l}$ scales with $1 - \frac{n^{l-1} + n^{l}}{n^{l-1}*n^{l}}$ , where $n^{l}$ denotes the number of neurons in layer $l$.</li><li><strong>Erdos-Renyi-Kernel(ERK):</strong> Modifies the original Erdos-Renyi formulation by including the kernel dimensions in the scaling factors. In other words, the number of parameters of the sparse convolutional layers are scaled proportional to equation - (1) .Here, $w^{l}$ and $h^{l}$ are the width and height of the $l^{th}$ convolutional kernel.</li></ol><p>$$1 - \frac{n^{l-1} + n^{l} + w^{l} + h^l}{n^{l-1}*n^{l}*w^{l}*h^{l}} , , , , , ,(1)$$</p><h3 id=update-schedule>Update Schedule
<a href=#update-schedule><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h3><p>The update schedule is defined by the following parameters:</p><ol><li>$\Delta T$: The number of iterations between sparse connectivity updates</li><li>$T_{end}$ : The iteration at which to stop updating the sparse connectivity</li><li>$\alpha$ : The initial fraction of connections updated</li><li>$f_{decay}$ : a function invoked every $\Delta T$ iterations until $T_{end}$, possibly decaying the fraction of updated connections over time.</li></ol><p>$$f_{decay}(t , ; \alpha, T_{end} ) = \frac{\alpha}{2} \left ( 1 + cos(\frac{t \pi}{T_{end}}) \right )$$</p><h3 id=drop-criterion>Drop Criterion
<a href=#drop-criterion><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h3><p>Every $\Delta {T}$, steps we drop the connections given by</p><p>$$ArgTopK(-|\theta^{l}|, (1 - s^{l})N^{l})$$</p><p>where $ArgTopK(v,k)$ gives the indices of the top-$k$ elements of vector $v$.</p><h3 id=grow-criterion>Grow Criterion
<a href=#grow-criterion><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h3><p>We grow the connections with highest magnitude gradients. Newly activated connections are <strong>initialized to zero</strong> and therefore don&rsquo;t affect the output of the network.</p></section><footer class=post-footer><div class="post-pagination dt w-100 mt4 mb2"><a class="next dtc pl2 tr v-top fw6" href=https://sauravmaheshkar.github.io/blog/compression-series/02/02-optimal-brain-damage/>Optimal Brain Damage &rarr;</a></div></footer></article></section></main><aside class=page-sidebar role=complementary><img src=/blog/compression-series/sidebar-compression.jpg class="db ma0"><div class="blog-info ph4 pt4 pb4 pb0-l"><h1 class=f3>Compression Series</h1><p class="f6 lh-copy measure">Paper Summaries and Notes on Neural Network Compression Techniques</p><p class="f7 measure lh-copy i mh0-l">Written by Saurav Maheshkar</p></div><div class="flex items-start sticky ph4 pb4"><div class="w-two-thirds w-50-l ph0"><h2 class="mt3 mb3 f5 fw7 ttu tracked"><a class="no-underline dim" href=/blog/compression-series/>In this series</a></h2><nav id=SeriesTableOfContents><ul><li class=active hugo-nav=/blog/compression-series/01/01-rigging-the-lottery/><a href=https://sauravmaheshkar.github.io/blog/compression-series/01/01-rigging-the-lottery/>Rigging the Lottery</a></li><li hugo-nav=/blog/compression-series/02/02-optimal-brain-damage/><a href=https://sauravmaheshkar.github.io/blog/compression-series/02/02-optimal-brain-damage/>Optimal Brain Damage</a></li></ul></nav></div><details open id=PageTableOfContents><summary><h2 class="mv0 f5 fw7 ttu tracked dib">On this page</h2></summary><div class="pl2 pr0 mh0"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#current-limitations>Current Limitations</a></li><li><a href=#the-rigl-method>The RigL Method</a></li></ul></nav></div></details></div></aside><footer class="site-footer pv4 bt b--transparent ph5" role=contentinfo><nav class="db dt-l w-100"><p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">&copy; 2021 Weights and Biases, New Delhi
<span class=middot-divider></span>Made with <span xmlns:dct=http://purl.org/dc/terms/ property="dct:title"><a xmlns:dct=http://purl.org/dc/terms/ href=https://github.com/hugo-apero/ rel=dct:source>Hugo Ap√©ro</a></span>.<br>Based on <span xmlns:dct=http://purl.org/dc/terms/ property="dct:title"><a xmlns:dct=http://purl.org/dc/terms/ href=https://github.com/formspree/blogophonic-hugo rel=dct:source>Blogophonic</a></span> by <a xmlns:cc=http://creativecommons.org/ns# href=https://formspree.io property="cc:attributionName" rel=cc:attributionurl>Formspree</a>.</p><div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0"><div class=social-icon-links aria-hidden=true><a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=https://github.com/SauravMaheshkar title=github target=_blank rel=noopener><i class="fab fa-github fa-lg fa-fw"></i></a>
<a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=https://www.instagram.com/sauravvmaheshkar/ title=instagram target=_blank rel=noopener><i class="fab fa-instagram fa-lg fa-fw"></i></a>
<a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=https://twitter.com/MaheshkarSaurav title=twitter target=_blank rel=noopener><i class="fab fa-twitter fa-lg fa-fw"></i></a>
<a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=https://orcid.org/0000-0002-5371-4651 title=orcid target=_blank rel=noopener><i class="ai ai-orcid fa-lg fa-fw"></i></a>
<a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=/blog/index.xml title=rss><i class="fas fa-rss fa-lg fa-fw"></i></a></div></div><div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0"><a class="dib pv1 ph2 link" href=/license/ title=License>License</a>
<a class="dib pv1 ph2 link" href=/contact/ title="Contact form">Contact</a></div></nav></footer></div></body></html>