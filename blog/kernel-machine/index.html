<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.83.1"><title>Every Model is a Kernel Machine | Saurav Maheshkar</title><meta property="twitter:site" content="@MaheshkarSaurav"><meta property="twitter:creator" content="@MaheshkarSaurav"><meta name=description content="Notes on the paper Every Model Learned by Gradient Descent Is Approximately a Kernel Machine by Pedro Domingos"><meta property="og:site_name" content="Saurav Maheshkar"><meta property="og:title" content="Every Model is a Kernel Machine | Saurav Maheshkar"><meta property="og:description" content="Notes on the paper Every Model Learned by Gradient Descent Is Approximately a Kernel Machine by Pedro Domingos"><meta property="og:type" content="page"><meta property="og:url" content="https://sauravm.netlify.app/blog/kernel-machine/"><meta property="og:locale" content="en"><meta property="og:image" content="https://sauravm.netlify.app/blog/kernel-machine/featured.jpg"><meta property="twitter:card" content="summary_large_image"><meta name=twitter:image content="https://sauravm.netlify.app/blog/kernel-machine/featured.jpg"><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta itemprop=name content="Every Model is a Kernel Machine"><meta itemprop=description content="Introduction     We all know how poorly understood deep learning is, in recent times fields such as Explainability and Interpretability have emerged highlighting the black box that are deep nets. In contrast, kernel machines are based on a well-developed mathematical theory but their performance lacks behind that of deep networks. The most used algorithm for training deep networks is undoubtedly Gradient Descent. In this paper the author shows that every model learnt using this method, regardless of its architecture, is approximately equivalent tp a kernel machine ü§î."><meta itemprop=datePublished content="2021-02-19T00:00:00+00:00"><meta itemprop=dateModified content="2021-02-19T00:00:00+00:00"><meta itemprop=wordCount content="859"><meta itemprop=image content="https://sauravm.netlify.app/blog/kernel-machine/featured.jpg"><meta itemprop=keywords content><!--[if IE]><script src=//html5shiv.googlecode.com/svn/trunk/html5.js></script><![endif]--><link rel="shortcut icon" href=/img/favicon.ico type=image/x-icon><link rel=icon href=/img/favicon.ico type=image/x-icon><link rel=stylesheet href=/style.main.min.9fe1917e3fd8cb643bf35aa0995479b67c8cee6eeca071b3bbe20d3248308e3b.css integrity="sha256-n+GRfj/Yy2Q781qgmVR5tnyM7m7soHGzu+INMkgwjjs=" media=screen><script src=/panelset.min.d74e921a1b9af2d938fdff19e433ba539cdb202961eddae2356a54199f0653ec.js type=text/javascript></script><script src=/main.min.38a0323c5b0bbb611c4874ba2d8fdaba57d20cc2b0d704b30250c235ba8b6d49.js type=text/javascript></script></head><body><div class=grid-container><header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role=banner><nav class="site-nav db dt-l w-100" role=navigation><a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href=https://sauravm.netlify.app/ title=Home><img src=/img/android-chrome-256x256.png class="dib db-l h2 w-auto" alt="Saurav Maheshkar"></a><div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked"><a class="link f6 f5-l dib pv1 ph2" href=/about/ title=About>About</a>
<a class="link f6 f5-l dib pv1 ph2 active" href=/blog/ title=Blog>Blog</a>
<a class="link f6 f5-l dib pv1 ph2" href=/project/ title="Project Portfolio">Projects</a>
<a class="link f6 f5-l dib pv1 ph2" href=https://sauravmaheshkar.substack.com/welcome title=Newsletter>Newsletter</a></div></nav></header><main class="page-main pa4" role=main><section class="page-content mw7 center"><article class="post-content pa0 ph4-l"><header class=post-header><h1 class="f1 lh-solid measure-narrow mb3 fw4">Every Model is a Kernel Machine</h1><p class="f6 measure lh-copy mv1">By Saurav Maheshkar in <a href=https://sauravm.netlify.app/categories/paper-summaries>Paper Summaries</a></p><p class="f7 db mv0 ttu">February 19, 2021</p></header><section class="post-body pt5 pb4"><h2 id=introduction>Introduction
<a href=#introduction><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>We all know how poorly understood deep learning is, in recent times fields such as Explainability and Interpretability have emerged highlighting the black box that are deep nets. In contrast, kernel machines are based on a well-developed mathematical theory but their performance lacks behind that of deep networks. The most used algorithm for training deep networks is undoubtedly Gradient Descent. In this paper the author shows that every model learnt using this method, regardless of its architecture, is approximately equivalent tp a kernel machine ü§î. This kernel learns the similarity of the model at two data points in the neighborhood of the path taken by the model parameters during learning. Kernel machines store a subset of the training data points and match them to the query using the kernel and thus deep network weights are nothing but a superposition of the training data points in the kernel&rsquo;s feature space ü§® ü§®.</p><p>This contrasts with the traditional school of thought üßê that deep learning is a method for discovering representations from data.</p><h2 id=path-kernels>Path Kernels
<a href=#path-kernels><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>A kernel machine is of the form:</p><p>$$ y = g( \sum_{i} a_iK(x,x_i) + b ) $$</p><p>where</p><ul><li>$ x $ is a query data point,</li><li>The sum is over all training data points $ x_i $</li><li>$ g $ is an optional nonlinearity</li><li>$ a_i $ and $ b $ are learned parameters</li><li>The kernel $ K $ measures the similarity of its arguments</li></ul><p>For instance in supervised learning, $ a_i $ is typically a linear function of $ \hat{ y_{i} } $ (the known output of $ x_i $). Kernel machines, are also known as support vector machines.</p><p>But whether a representable function is learned or not depends on the learning algorithm. Most of us are familiar with the following:</p><p>$$ w_{s+1} = w_{s} - \epsilon\nabla_{w}L(w_{s}) $$</p><p>where:</p><ul><li>$ L = \sum_{i} L(\hat{ y_{i} }, y_{i}) $ is the loss function</li><li>$ w $ are the model parameters</li><li>$ \epsilon $ is the learning rate</li></ul><p>This process terminates when the gradient becomes zero and we reach the saddle point.</p><p>The author terms <strong>path kernels</strong> to be those kernel machines which result from gradient descent. Assuming the learning rate to be infinitesimally small, the path kernel between any two arbitrary data points is simply the dot product of the model&rsquo;s gradients at the two points over the path taken by the parameters during gradient descent:</p><p>$$ K(x,x^{'}) = \int_{c(t)} \nabla_{w}y(x).\nabla_{w}y(x^{'}) dt $$</p><p>where $ c(t) $ is the path. Intuitively, the path kernel measures how similarly the model at the two data points varies during learning. The more similar the variation for $ x $ and $ x_{'} $, the higher the weight of $ x_{'} $ in predicting $y$.</p><h2 id=some-definitions>Some Definitions
<a href=#some-definitions><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><ol><li><strong>Tangent Kernel:</strong> The Tangent Kernel associated with function $f_w(x)$ and the parameter vector $v$ is:</li></ol><p>$$
K_{f,v}^{g}(x, x^{'}) = \nabla_{w}f_w(x) . \nabla_wf_w(x^{'}) - (I)
$$</p><ol start=2><li><strong>Path Kernel:</strong> The path kernel associated with function $ f_w(x) $ and curve $c(t)$ in parameter space is:</li></ol><p>$$
K_{f,c}^{p}(x, x^{'}) = \int_{c(t)}^{} K_{f,v}^{g}(x, x^{'}) dt - (II)
$$</p><h2 id=the-proof>The Proof
<a href=#the-proof><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>As per Gradient Descent:</p><p>$$
w_{s+1} = w_{s} - \epsilon\nabla_{w}L(w_{s})
$$</p><p>$$
\frac{ w_{s+1} - w_{s} }{ \epsilon } = - \nabla_{w}L(w_{s})
$$</p><p>This becomes a differential equation of the form(Also known as <span style=color:blue>Gradient Flow</span>.):</p><p>$$
\frac{dw(t)}{dt} = - \nabla_{w}L(w(t)) - (1)
$$</p><p>Then for any differentiable function of the weights $y$ (<span style=color:green>Using Chain Rule</span>)</p><p>$$
\frac{dy}{dt} = \sum_{j=1}^{d} \frac{ \partial{y}}{ \partial{w_j} } \frac{\partial{w_j}}{\partial{t}}
$$</p><p>Here, $d$ is the number of parameters. Replacing $dw_j/dt$, by it&rsquo;s value from ${eq}^{n} , (1)$, we get:</p><p>$$
\frac{dy}{dt} = \sum_{j=1}^{d} \frac{ \partial{y}}{ \partial{w_j} } \left (- \frac{\partial{L}}{\partial{w_j}} \right)
$$</p><p>Applying <span style=color:green>Chain Rule</span> and <span style=color:green>additivity of the loss</span>, we get:</p><p>$$
\frac{dy}{dt} = \sum_{j=1}^{d} \frac{ \partial{y}}{ \partial{w_j} } \left (- \sum_{i = 1}^{m} \frac{\partial{L}}{\partial{y_i}} \frac{\partial{y_i}}{\partial{w_j}} \right)
$$</p><p>Rearranging the terms a little bit, we get:</p><p>$$
\frac{dy}{dt} = \sum_{i = 1}^{m} \frac{\partial{L}}{\partial{y_i}} \left (- \sum_{j=1}^{d} \frac{ \partial{y}}{ \partial{w_j} } \frac{\partial{y_i}}{\partial{w_j}} \right)
$$</p><p>Let $ L^{'}(y_{i}^{*}, y_{i}) = \partial{L} / \partial{y_i} $ (<span style=color:green>The loss derivative for the ith output</span>). Applying this and **Definition(I)** (<span style=color:green>Tangent Kernel</span>):</p><p>$$
\frac{dy}{dt} = \sum_{i = 1}^{m} L^{'}(y_{i}^{*}, y_{i}) K_{f,w(t)}^{g}(x, x^{'})
$$</p><p>If $ y_0 $ is the initial model, prior to the gradient descent. Then for the final model $y$;</p><p>$$
\lim_{\epsilon \rightarrow 0} y = y_0 - \int_{c(t)} \sum_{i = 1}^{m} L^{'}(y_{i}^{*}, y_{i}) K_{f,w(t)}^{g}(x, x^{'})
$$</p><p>where $c(t)$ is the path taken by the parameters during gradient descent. Multiplying and dividing by $\int_{c(t)} K_{f,w(t)}^{g}(x, x^{'}) dt$, we get:</p><p>$$
\lim_{\epsilon \rightarrow 0} y = y_0 - \int_{c(t)} \sum_{i = 1}^{m} \left( \frac{ \int_{c(t)} K_{f,w(t)}^{g}(x, x^{'}) dt L^{'}(y_{i}^{*}, y_{i})dt}{\int_{c(t)} K_{f,w(t)}^{g}(x, x^{'}) dt} \right) K_{f,w(t)}^{g}(x, x^{'})
$$</p><p>Let the term in the braces $()$ be $\bar{L^{'}}$ i.e., <span style=color:green>the average loss derivative weighted by similarity to x</span>. Applying this and definition (II), we get:</p><p>$$
\lim_{\epsilon \rightarrow 0} y = y_0 - \int_{c(t)} \sum_{i = 1}^{m} \bar{L^{'}(y_{i}^{*}, y_{i}) K_{f,c}^{p}(x, x^{'})}
$$</p><p>Thus, finally we get</p><p>$$
\lim_{\epsilon \rightarrow 0} y = \sum_{i=1}^{m} a_i K(x, x_i) + b
$$</p><p>where,</p><ul><li>$ b = y_0 $</li><li>$ a = -\bar{L^{'}}(y_{i}^{*}, y_i) $</li><li>$ K(x, x_i) = K_{f,c}^{p} (x, x_i) $</li></ul><details closed class="f6 fw7 input-reset"><dl class="f6 lh-copy"><dt class=fw7>Posted on:</dt><dd class="fw5 ml0">February 19, 2021</dd></dl><dl class="f6 lh-copy"><dt class=fw7>Length:</dt><dd class="fw5 ml0">5 minute read, 859 words</dd></dl><dl class="f6 lh-copy"><dt class=fw7>Categories:</dt><dd class="fw5 ml0"><a href=https://sauravm.netlify.app/categories/paper-summaries>Paper Summaries</a></dd></dl><dl class="f6 lh-copy"><dt class=fw7>See Also:</dt></dl></details></section><footer class=post-footer><div class="post-pagination dt w-100 mt4 mb2"><a class="next dtc pl2 tr v-top fw6" href=https://sauravm.netlify.app/blog/duck-typing-eafp/>Introduction to Duck Typing and EAFP &rarr;</a></div></footer></article><div class="post-comments pa0 pa4-l mt4"><script src=https://utteranc.es/client.js repo=apreshill/apero issue-term=pathname theme=boxy-light label="comments :crystal_ball:" crossorigin=anonymous async type=text/javascript></script></div></section></main><footer class="site-footer pv4 bt b--transparent ph5" role=contentinfo><nav class="db dt-l w-100"><p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">&copy; 2021 Weights and Biases, New Delhi
<span class=middot-divider></span>Made with <span xmlns:dct=http://purl.org/dc/terms/ property="dct:title"><a xmlns:dct=http://purl.org/dc/terms/ href=https://github.com/hugo-apero/ rel=dct:source>Hugo Ap√©ro</a></span>.<br>Based on <span xmlns:dct=http://purl.org/dc/terms/ property="dct:title"><a xmlns:dct=http://purl.org/dc/terms/ href=https://github.com/formspree/blogophonic-hugo rel=dct:source>Blogophonic</a></span> by <a xmlns:cc=http://creativecommons.org/ns# href=https://formspree.io property="cc:attributionName" rel=cc:attributionurl>Formspree</a>.</p><div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0"><div class=social-icon-links aria-hidden=true><a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=https://github.com/SauravMaheshkar title=github target=_blank rel=noopener><i class="fab fa-github fa-lg fa-fw"></i></a>
<a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=https://www.instagram.com/sauravvmaheshkar/ title=instagram target=_blank rel=noopener><i class="fab fa-instagram fa-lg fa-fw"></i></a>
<a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=https://twitter.com/MaheshkarSaurav title=twitter target=_blank rel=noopener><i class="fab fa-twitter fa-lg fa-fw"></i></a>
<a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=https://orcid.org/0000-0002-5371-4651 title=orcid target=_blank rel=noopener><i class="ai ai-orcid fa-lg fa-fw"></i></a>
<a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href=/blog/index.xml title=rss><i class="fas fa-rss fa-lg fa-fw"></i></a></div></div><div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0"><a class="dib pv1 ph2 link" href=/license/ title=License>License</a>
<a class="dib pv1 ph2 link" href=/contact/ title="Contact form">Contact</a></div></nav></footer></div></body></html>