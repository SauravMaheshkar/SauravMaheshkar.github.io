<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Project Portfolio on Saurav Maheshkar</title><link>https://sauravm.netlify.app/project/</link><description>Recent content in Project Portfolio on Saurav Maheshkar</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 31 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://sauravm.netlify.app/project/index.xml" rel="self" type="application/rss+xml"/><item><title>Stellarflare</title><link>https://sauravm.netlify.app/project/stellarflare/</link><pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/project/stellarflare/</guid><description> Tensorflow Based Framework for detecting Stellar Flares</description></item><item><title>X Ray Image Analysis</title><link>https://sauravm.netlify.app/project/xray-image-analysis/</link><pubDate>Sat, 10 Apr 2021 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/project/xray-image-analysis/</guid><description>In this project, I went over the entire pipeline of creating a Binary Image Classifier using Tensorflow. I covered, all aspects of the pipeline such as experimenting with different network architectures and comparing metrics, pruning and quantization of the model for faster inference and finally two methods of deploying such models. I trained a Convolutional Neural Network(Efficient Net) to recognize chest X-rays of people with pneumonia and then created a interactive web application.</description></item><item><title>Compressed DNNs Forget</title><link>https://sauravm.netlify.app/project/bias-compressed/</link><pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate><guid>https://sauravm.netlify.app/project/bias-compressed/</guid><description>Reproducibility Study Current state-of-the-art models are famously huge and over-parameterized––in fact, they contain way more parameters than the number of data points in the dataset. But in many ways, over-parameterization is behind the success of modern-day deep learning. Think about something like Switch Transformer having a trillion parameters or Vision Transformer-Huge having 632M parameters. These models require enormous amounts of computation and memory and that not only increases the infrastructure costs, but also makes deployment to resource-constrained environments such as mobile phones or smart devices challenging.</description></item></channel></rss>